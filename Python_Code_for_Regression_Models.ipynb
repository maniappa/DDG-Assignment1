{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maniappa/DDG-Assignment1/blob/main/Python_Code_for_Regression_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "\n",
        "# --- Load and Clean Data ---\n",
        "# Load the dataset from the CSV file.\n",
        "# A try-except block is used to handle the case where the file is not found.\n",
        "try:\n",
        "    df = pd.read_csv('WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: The dataset file 'WA_Fn-UseC_-Telco-Customer-Churn.csv' was not found.\")\n",
        "    # Exit if the file doesn't exist, as the rest of the script depends on it.\n",
        "    exit()\n",
        "\n",
        "# --- Initial Data Cleaning for 'TotalCharges' ---\n",
        "# The 'TotalCharges' column may contain empty spaces for new customers.\n",
        "# 1. Convert the column to a numeric type. `errors='coerce'` will turn any non-numeric values into NaN (Not a Number).\n",
        "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
        "# 2. Fill the resulting NaN values with 0. This is a logical imputation as new customers have 0 total charges.\n",
        "df['TotalCharges'].fillna(0, inplace=True)\n",
        "\n",
        "# Drop the customerID column as it is just an identifier and has no predictive value.\n",
        "df = df.drop('customerID', axis=1)\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# PART 1: PREDICTING CUSTOMER CHURN (LOGISTIC REGRESSION)\n",
        "# ==============================================================================\n",
        "print(\"--- Part 1: Predicting Customer Churn with Logistic Regression ---\")\n",
        "\n",
        "# --- 1. Data Preparation for Churn Prediction ---\n",
        "\n",
        "# Define the target variable (y) and features (X).\n",
        "X_churn = df.drop('Churn', axis=1)\n",
        "y_churn = df['Churn'].apply(lambda x: 1 if x == 'Yes' else 0) # Convert 'Yes'/'No' to 1/0\n",
        "\n",
        "# Identify numerical and categorical feature names\n",
        "numerical_features_churn = X_churn.select_dtypes(include=np.number).columns.tolist()\n",
        "categorical_features_churn = X_churn.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "# Create a preprocessing pipeline using ColumnTransformer.\n",
        "# This ensures that the same transformations are applied consistently.\n",
        "preprocessor_churn = ColumnTransformer(\n",
        "    transformers=[\n",
        "        # 'num' pipeline: Applies StandardScaler to numerical features.\n",
        "        ('num', StandardScaler(), numerical_features_churn),\n",
        "        # 'cat' pipeline: Applies OneHotEncoder to categorical features.\n",
        "        # handle_unknown='ignore' prevents errors if new categories appear in test data.\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features_churn)\n",
        "    ])\n",
        "\n",
        "# Split the data into training (80%) and testing (20%) sets.\n",
        "# stratify=y_churn ensures the proportion of churned customers is the same in both sets.\n",
        "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(\n",
        "    X_churn, y_churn, test_size=0.2, random_state=42, stratify=y_churn\n",
        ")\n",
        "\n",
        "# --- 2. Build and Train the Logistic Regression Model ---\n",
        "\n",
        "# Create the full model pipeline by chaining the preprocessor and the classifier.\n",
        "log_reg_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor_churn),\n",
        "    ('classifier', LogisticRegression(random_state=42, max_iter=1000))\n",
        "])\n",
        "\n",
        "# Train the model on the training data.\n",
        "log_reg_pipeline.fit(X_train_c, y_train_c)\n",
        "\n",
        "# --- 3. Evaluate the Churn Model ---\n",
        "# Make predictions on the unseen test data.\n",
        "y_pred_c = log_reg_pipeline.predict(X_test_c)\n",
        "y_pred_proba_c = log_reg_pipeline.predict_proba(X_test_c)[:, 1] # Probabilities for the '1' class\n",
        "\n",
        "# Calculate and print the classification metrics.\n",
        "print(\"\\nLogistic Regression Model Evaluation:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test_c, y_pred_c):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test_c, y_pred_c):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test_c, y_pred_c):.4f}\")\n",
        "print(f\"F1-Score: {f1_score(y_test_c, y_pred_c):.4f}\")\n",
        "print(f\"ROC AUC Score: {roc_auc_score(y_test_c, y_pred_proba_c):.4f}\")\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test_c, y_pred_c))\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# PART 2: PREDICTING TOTAL CHARGES (LINEAR REGRESSION)\n",
        "# ==============================================================================\n",
        "print(\"\\n\\n--- Part 2: Predicting Total Charges with Linear Regression ---\")\n",
        "\n",
        "# --- 1. Data Preparation for Total Charges Prediction ---\n",
        "\n",
        "# Define the target (y) and features (X). 'Churn' is dropped from features.\n",
        "X_charges = df.drop(['TotalCharges', 'Churn'], axis=1)\n",
        "y_charges = df['TotalCharges']\n",
        "\n",
        "# Identify numerical and categorical feature names for this new feature set.\n",
        "numerical_features_charges = X_charges.select_dtypes(include=np.number).columns.tolist()\n",
        "categorical_features_charges = X_charges.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "# Create the preprocessor for the regression task.\n",
        "preprocessor_charges = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_features_charges),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features_charges)\n",
        "    ])\n",
        "\n",
        "# Split the data into training and testing sets.\n",
        "X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(\n",
        "    X_charges, y_charges, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# --- 2. Build and Train the Linear Regression Model ---\n",
        "\n",
        "# Create the full model pipeline.\n",
        "lin_reg_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor_charges),\n",
        "    ('regressor', LinearRegression())\n",
        "])\n",
        "\n",
        "# Train the model.\n",
        "lin_reg_pipeline.fit(X_train_r, y_train_r)\n",
        "\n",
        "# --- 3. Evaluate the Total Charges Model ---\n",
        "# Make predictions on the test data.\n",
        "y_pred_r = lin_reg_pipeline.predict(X_test_r)\n",
        "\n",
        "# Calculate and print the regression metrics.\n",
        "r2 = r2_score(y_test_r, y_pred_r)\n",
        "mse = mean_squared_error(y_test_r, y_pred_r)\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "print(\"\\nLinear Regression Model Evaluation:\")\n",
        "print(f\"R-squared (R²): {r2:.4f}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): ${rmse:.2f}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-1521930399>:26: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['TotalCharges'].fillna(0, inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Part 1: Predicting Customer Churn with Logistic Regression ---\n",
            "\n",
            "Logistic Regression Model Evaluation:\n",
            "Accuracy: 0.8055\n",
            "Precision: 0.6572\n",
            "Recall: 0.5588\n",
            "F1-Score: 0.6040\n",
            "ROC AUC Score: 0.8421\n",
            "\n",
            "Confusion Matrix:\n",
            "[[926 109]\n",
            " [165 209]]\n",
            "\n",
            "\n",
            "--- Part 2: Predicting Total Charges with Linear Regression ---\n",
            "\n",
            "Linear Regression Model Evaluation:\n",
            "R-squared (R²): 0.9050\n",
            "Mean Squared Error (MSE): 494452.12\n",
            "Root Mean Squared Error (RMSE): $703.17\n"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57C-qAe-LkLj",
        "outputId": "5ec2a86a-8870-49a2-c506-407518550c3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report\n",
        "\n",
        "# --- Load and Clean Data ---\n",
        "try:\n",
        "    df = pd.read_csv('WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: The dataset file 'WA_Fn-UseC_-Telco-Customer-Churn.csv' was not found.\")\n",
        "    exit()\n",
        "\n",
        "# Clean 'TotalCharges' column\n",
        "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
        "df['TotalCharges'].fillna(0, inplace=True)\n",
        "df = df.drop('customerID', axis=1)\n",
        "\n",
        "# --- 1. Data Preparation for Churn Prediction ---\n",
        "# This setup is the same as the baseline model\n",
        "X = df.drop('Churn', axis=1)\n",
        "y = df['Churn'].apply(lambda x: 1 if x == 'Yes' else 0)\n",
        "\n",
        "numerical_features = X.select_dtypes(include=np.number).columns.tolist()\n",
        "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "    ])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# --- 2. Define the Model Pipeline ---\n",
        "# We define the pipeline that will be used in the grid search.\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(random_state=42, max_iter=1000))\n",
        "])\n",
        "\n",
        "# --- 3. Define the Hyperparameter Grid for Tuning ---\n",
        "# These are the 'dials' we will tune for the Logistic Regression model.\n",
        "# 'classifier__' prefix is used to specify that these parameters belong to the 'classifier' step of the pipeline.\n",
        "param_grid = {\n",
        "    'classifier__penalty': ['l1', 'l2'],\n",
        "    'classifier__C': [0.01, 0.1, 1, 10, 100],  # Inverse of regularization strength\n",
        "    'classifier__solver': ['liblinear']  # 'liblinear' is a good solver for this small dataset and works with both l1/l2\n",
        "}\n",
        "\n",
        "# --- 4. Perform Grid Search with Cross-Validation ---\n",
        "# We will search for the best parameters based on the 'f1' score, as it provides\n",
        "# a good balance between precision and recall for our imbalanced dataset.\n",
        "# cv=5 means 5-fold cross-validation.\n",
        "print(\"Starting hyperparameter tuning with GridSearchCV...\")\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='f1', n_jobs=-1, verbose=1)\n",
        "\n",
        "# Fit the grid search to the training data. This will test all parameter combinations.\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# --- 5. Display Tuning Results ---\n",
        "print(\"\\nHyperparameter tuning complete.\")\n",
        "print(f\"Best F1-score found during tuning: {grid_search.best_score_:.4f}\")\n",
        "print(\"Best parameters found:\")\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "# --- 6. Evaluate the Tuned Model vs. Baseline ---\n",
        "# Get the best model found by the grid search.\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set with the tuned model.\n",
        "y_pred_tuned = best_model.predict(X_test)\n",
        "\n",
        "# Baseline model (for comparison)\n",
        "baseline_model = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(random_state=42, max_iter=1000))\n",
        "])\n",
        "baseline_model.fit(X_train, y_train)\n",
        "y_pred_baseline = baseline_model.predict(X_test)\n",
        "\n",
        "# --- 7. Final Comparison ---\n",
        "print(\"\\n--- Model Performance Comparison ---\")\n",
        "\n",
        "baseline_metrics = {\n",
        "    \"Accuracy\": accuracy_score(y_test, y_pred_baseline),\n",
        "    \"Precision\": precision_score(y_test, y_pred_baseline),\n",
        "    \"Recall\": recall_score(y_test, y_pred_baseline),\n",
        "    \"F1-Score\": f1_score(y_test, y_pred_baseline)\n",
        "}\n",
        "\n",
        "tuned_metrics = {\n",
        "    \"Accuracy\": accuracy_score(y_test, y_pred_tuned),\n",
        "    \"Precision\": precision_score(y_test, y_pred_tuned),\n",
        "    \"Recall\": recall_score(y_test, y_pred_tuned),\n",
        "    \"F1-Score\": f1_score(y_test, y_pred_tuned)\n",
        "}\n",
        "\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Baseline Model': baseline_metrics,\n",
        "    'Tuned Model': tuned_metrics\n",
        "})\n",
        "\n",
        "print(comparison_df)\n",
        "\n",
        "print(\"\\n--- Detailed Report for Tuned Model ---\")\n",
        "print(classification_report(y_test, y_pred_tuned))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSUAwucbM6g6",
        "outputId": "16de6669-3146-486f-aa3f-18315b9bf9c5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting hyperparameter tuning with GridSearchCV...\n",
            "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-1039845496>:19: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['TotalCharges'].fillna(0, inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Hyperparameter tuning complete.\n",
            "Best F1-score found during tuning: 0.5994\n",
            "Best parameters found:\n",
            "{'classifier__C': 100, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear'}\n",
            "\n",
            "--- Model Performance Comparison ---\n",
            "           Baseline Model  Tuned Model\n",
            "Accuracy         0.805536     0.801278\n",
            "Precision        0.657233     0.647799\n",
            "Recall           0.558824     0.550802\n",
            "F1-Score         0.604046     0.595376\n",
            "\n",
            "--- Detailed Report for Tuned Model ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87      1035\n",
            "           1       0.65      0.55      0.60       374\n",
            "\n",
            "    accuracy                           0.80      1409\n",
            "   macro avg       0.75      0.72      0.73      1409\n",
            "weighted avg       0.79      0.80      0.80      1409\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}